{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Classification\n",
    "\n",
    "### We will explore Text Classification using nltk, scikit-learn, and gensim\n",
    "\n",
    "We will use a newsgroups dataset: https://scikit-learn.org/stable/datasets/index.html#the-20-newsgroups-text-dataset (more than 18000 newsgroup posts, across 20 news categories)\n",
    "\n",
    "#### Goal: Build ML models that predict the category of a newsgroup post, based on the text of the post\n",
    "\n",
    "Prof Evan Katsamakas,\n",
    "Gabelli School, 2/2019\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd  \n",
    "import nltk\n",
    "import gensim \n",
    "from sklearn.datasets import fetch_20newsgroups \n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get the data and take a look"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "categories = ['talk.politics.guns','rec.sport.baseball'] # We focus on 2 news categories\n",
    "def get_data():\n",
    "    data = fetch_20newsgroups(subset='all',\n",
    "                              shuffle=True,\n",
    "                              categories=categories,\n",
    "                              remove=('headers', 'footers', 'quotes'))\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['rec.sport.baseball', 'talk.politics.guns']\n",
      "Sample document: For those who didn't figure it out, the below message was a reply to another\n",
      "in sci.crypt, for which the poster put t.p.g. in the Followup-To line. I\n",
      "didn't notice that. Apologies to those who were confused.\n",
      "\n",
      "The substance makes little sense unless one reads the prior messages.\n",
      "\n",
      "However, I don't wish to enter into this discussion here, as it will be yet\n",
      "another rehearsal of a long-tired set of arguments. Suffice it to say that I\n",
      "disagree both with the interpretation of \"well-regulated\" in the Second\n",
      "Amendment offered by gun lovers, and what I think to be their distortion of\n",
      "the same phrase in the associated Federalist papers. My Webster and my\n",
      "reading of the language convinces me that the word meant both under control,\n",
      "and disciplined, and not 'of good marksmanship'. I think the latter a\n",
      "special interest pleading. No one has yet shown a contemporateous reference\n",
      "in which \"well regulated\" unambiguously meant 'of good marksmanship', and\n",
      "not under control/disciplined, etc.\n",
      "\n",
      "Thus I continue to believe the Second Amendment is a militia clause and not\n",
      "an 'arming everyone' clause. Others are welcome to disagree (as I know many\n",
      "do) and little would be served by rehashing this topic in this particular\n",
      "forum.\n",
      "\n",
      "To avoid flames, or unproductive rehashings, I note that I've come in here\n",
      "to post this one message, just to clarify the one below. I'm now outta here\n",
      "again though I'm available via e-mail.\n",
      "\n",
      "David\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Class label: 1\n",
      "Actual class label: talk.politics.guns\n"
     ]
    }
   ],
   "source": [
    "# get text data and their labels\n",
    "dataset = get_data()\n",
    "print(dataset.target_names)\n",
    "\n",
    "corpus, labels = dataset.data, dataset.target\n",
    "\n",
    "print('Sample document:', corpus[10])\n",
    "print('Class label:',labels[10])\n",
    "print('Actual class label:', dataset.target_names[labels[10]])\n",
    "\n",
    "# split training dataset and testing dataset\n",
    "train_corpus, test_corpus, train_labels, test_labels = train_test_split(corpus,labels,test_size=0.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepape features for ML \n",
    "### {bow, tfidf, word2vec}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#bow features\n",
    "from sklearn.feature_extraction.text import CountVectorizer #tokenizes and counts words\n",
    "\n",
    "# build bag of words features' vectorizer and get features\n",
    "bow_vectorizer=CountVectorizer(min_df=1, ngram_range=(1,1))\n",
    "bow_train_features = bow_vectorizer.fit_transform(train_corpus)\n",
    "bow_test_features = bow_vectorizer.transform(test_corpus) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tfidf features\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer #alternatively, use TfidfTransformer()\n",
    "\n",
    "tfidf_vectorizer=TfidfVectorizer(min_df=1, \n",
    "                                 norm='l2',\n",
    "                                 smooth_idf=True,\n",
    "                                 use_idf=True,\n",
    "                                 ngram_range=(1,1))\n",
    "tfidf_train_features = tfidf_vectorizer.fit_transform(train_corpus)  \n",
    "tfidf_test_features = tfidf_vectorizer.transform(test_corpus)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenize documents for word2vec\n",
    "tokenized_train = [nltk.word_tokenize(text)\n",
    "                   for text in train_corpus]\n",
    "tokenized_test = [nltk.word_tokenize(text)\n",
    "                   for text in test_corpus]  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build word2vec model                   \n",
    "wv_model = gensim.models.Word2Vec(tokenized_train,\n",
    "                               size=200,                          #set the size or dimension for the word vectors \n",
    "                               window=60,                        #specify the length of the window of words taken as context\n",
    "                               min_count=10)                   #ignores all words with total frequency lower than                     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def average_word_vectors(words, model, vocabulary, num_features):\n",
    "    \n",
    "    feature_vector = np.zeros((num_features,),dtype=\"float64\")\n",
    "    nwords = 0.\n",
    "    \n",
    "    for word in words:\n",
    "        if word in vocabulary: \n",
    "            nwords = nwords + 1.\n",
    "            feature_vector = np.add(feature_vector, model[word])\n",
    "    \n",
    "    if nwords:\n",
    "        feature_vector = np.divide(feature_vector, nwords)\n",
    "        \n",
    "    return feature_vector \n",
    "   \n",
    "\n",
    "def averaged_word_vectorizer(corpus, model, num_features):\n",
    "    vocabulary = set(model.wv.index2word)\n",
    "    features = [average_word_vectors(tokenized_sentence, model, vocabulary, num_features)\n",
    "                    for tokenized_sentence in corpus]\n",
    "    return np.array(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\DELL\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:9: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  if __name__ == '__main__':\n"
     ]
    }
   ],
   "source": [
    "# averaged word vector features from word2vec\n",
    "avg_wv_train_features = averaged_word_vectorizer(corpus=tokenized_train,\n",
    "                                                 model=wv_model,\n",
    "                                                 num_features=200)                   \n",
    "avg_wv_test_features = averaged_word_vectorizer(corpus=tokenized_test,\n",
    "                                                model=wv_model,\n",
    "                                                num_features=200) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define metrics for evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "\n",
    "# define a function to evaluate our classification models based on four metrics\n",
    "def get_metrics(true_labels, predicted_labels):\n",
    "    \n",
    "    print ('Accuracy:', np.round(                                                    \n",
    "                        metrics.accuracy_score(true_labels, \n",
    "                                               predicted_labels),\n",
    "                        2))\n",
    "    print ('Precision:', np.round(\n",
    "                        metrics.precision_score(true_labels, \n",
    "                                               predicted_labels),\n",
    "                        2))\n",
    "    print ('Recall:', np.round(\n",
    "                        metrics.recall_score(true_labels, \n",
    "                                               predicted_labels),\n",
    "                        2))\n",
    "    print ('F1 Score:', np.round(\n",
    "                        metrics.f1_score(true_labels, \n",
    "                                               predicted_labels),\n",
    "                        2))\n",
    "                        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define how to train and evaluate classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a function that trains the model, performs predictions and evaluates the predictions\n",
    "def train_predict_evaluate_model(classifier, \n",
    "                                 train_features, train_labels, \n",
    "                                 test_features, test_labels):\n",
    "    # build model    \n",
    "    classifier.fit(train_features, train_labels)\n",
    "    # predict using model\n",
    "    predictions = classifier.predict(test_features) \n",
    "    # evaluate model prediction performance   \n",
    "    get_metrics(true_labels=test_labels, \n",
    "                predicted_labels=predictions)\n",
    "    return predictions    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train and evaluate {mnb, svm} with bow features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.96\n",
      "Precision: 0.97\n",
      "Recall: 0.94\n",
      "F1 Score: 0.96\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "\n",
    "mnb = MultinomialNB()\n",
    "svm = SGDClassifier(loss='hinge', max_iter=100)\n",
    "\n",
    "# Multinomial Naive Bayes with bag of words features\n",
    "mnb_bow_predictions = train_predict_evaluate_model(classifier=mnb,\n",
    "                                           train_features=bow_train_features,\n",
    "                                           train_labels=train_labels,\n",
    "                                           test_features=bow_test_features,\n",
    "                                           test_labels=test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.92\n",
      "Precision: 0.96\n",
      "Recall: 0.88\n",
      "F1 Score: 0.92\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\DELL\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:183: FutureWarning: max_iter and tol parameters have been added in SGDClassifier in 0.19. If max_iter is set but tol is left unset, the default value for tol in 0.19 and 0.20 will be None (which is equivalent to -infinity, so it has no effect) but will change in 0.21 to 1e-3. Specify tol to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "#Support Vector Machine with bag of words features\n",
    "svm_bow_predictions = train_predict_evaluate_model(classifier=svm,\n",
    "                                           train_features=bow_train_features,\n",
    "                                           train_labels=train_labels,\n",
    "                                           test_features=bow_test_features,\n",
    "                                           test_labels=test_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train and evaluate {svm} with {tfidf, word2vec} features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.93\n",
      "Precision: 0.98\n",
      "Recall: 0.88\n",
      "F1 Score: 0.93\n"
     ]
    }
   ],
   "source": [
    "# Support Vector Machine with tfidf features\n",
    "svm_tfidf_predictions = train_predict_evaluate_model(classifier=svm,\n",
    "                                           train_features=tfidf_train_features,\n",
    "                                           train_labels=train_labels,\n",
    "                                           test_features=tfidf_test_features,\n",
    "                                           test_labels=test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.77\n",
      "Precision: 0.7\n",
      "Recall: 0.93\n",
      "F1 Score: 0.8\n"
     ]
    }
   ],
   "source": [
    "# Support Vector Machine with averaged word vector features\n",
    "svm_avgwv_predictions = train_predict_evaluate_model(classifier=svm,\n",
    "                                           train_features=avg_wv_train_features,\n",
    "                                           train_labels=train_labels,\n",
    "                                           test_features=avg_wv_test_features,\n",
    "                                           test_labels=test_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CONFUSION MATRIX (for SVM TFIDF) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>167</td>\n",
       "      <td>115</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>19</td>\n",
       "      <td>271</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     0    1\n",
       "0  167  115\n",
       "1   19  271"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# build confusion matrix for SVM TF-IDF-based model\n",
    "cm = metrics.confusion_matrix(test_labels, svm_avgwv_predictions)\n",
    "pd.DataFrame(cm, index=range(0,2), columns=range(0,2))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rec.sport.baseball -> talk.politics.guns\n"
     ]
    }
   ],
   "source": [
    "# Observe false positive output\n",
    "class_names = dataset.target_names\n",
    "print (class_names[0], '->', class_names[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Actual Label: rec.sport.baseball\n",
      "Predicted Label: talk.politics.guns\n",
      "Document:-\n",
      "   Well Sherri, I'd agree with you except that most 'kiddies' have more sense than to spew their obscenities in front of a group of adults..  I try to edit this newsgroup and feed it to one of the local elementary schools, they have a group of students that just love baseball and are learning to use computers, but I'm telling you, it's gotten to the point that I don't even edit the files anymore, just read them and throw out the trash...  And thanks to all you people that think it's wonderful to include a swear word or two in your signature files, that's really nice...  I have to read the whole article and then toss it out because of the .sig.  Don't get me wrong, I know all the words you do, (and I've even made up some of my own!) or I wouldn't be able to edit them out ;^) but this just doesn't seem to be the place, a public forum, to spew foul language, sorry..  Thanks to all you people that keep in mind, there might be some decent, young people, interested in baseball and computers reading this newsgroup..  They enjoy your articles.\n",
      "Actual Label: rec.sport.baseball\n",
      "Predicted Label: talk.politics.guns\n",
      "Document:-\n",
      "          Good question!  I kind of wondered about this myself.  Just one thought I had on it  -  the Babe himself was IN \"Pride of the Yankees\", which made me think  a) that version may be closer to the truth, and  b) the Babe must have been a pretty good gut to be in the movie even though some of the scenes didn't make him look completely flattering. \n",
      "Actual Label: rec.sport.baseball\n",
      "Predicted Label: talk.politics.guns\n",
      "Document:-\n",
      "  Preliminary negotiations started already, I believe.  Though the word is that they are going slooooooooooooooooooooooowly.\n",
      "Actual Label: rec.sport.baseball\n",
      "Predicted Label: talk.politics.guns\n",
      "Document:-\n",
      " \tHeheheheh.. Sorry, Roger, I wronged you.. You're not boring ALL the time..  This one is a classic. 'cordially, as always' HEHEHEHEHHE!  \tAre you a jerk?  \t\tDo people hate you?  \t\t\tAre you no fun at parties?  \tWell! Come to the ROGER MAYNARD SCHOOL OF CORDIALITY!  \tWe can teach you to be 'cordial' with the best of them! Use such time honored 'cordiality' techniques as:  \t1) Calling people assholes! \t2) Comparing them to viruses!  \tFor advanced students:  \t3) Comparing them to DIRTY viruses. What is a DIRTY virus, \t\tand how can you tell it from a clean one? We know, \t\tand here at the ROGER MAYNARD SCHOOL OF CORDIALITY, \t\twe can teach you to know, too!  \tHEHEHEH.. Thanks, Roger.. This made my evening :-)\n"
     ]
    }
   ],
   "source": [
    "# Look at some misclassified documents in detail\n",
    "import re\n",
    "\n",
    "num = 0\n",
    "for document, label, predicted_label in zip(test_corpus, test_labels, svm_tfidf_predictions):\n",
    "    if label == 0 and predicted_label == 1:\n",
    "        print ('Actual Label:', class_names[label])\n",
    "        print ('Predicted Label:', class_names[predicted_label])\n",
    "        print ('Document:-')\n",
    "        print (re.sub('\\n', ' ', document))\n",
    "        num += 1\n",
    "        if num == 4:\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Another Model: Desicion Tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Train and evaluate {svm} by using decision tree model\n",
    "2. CONFUSION MATRIX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.82\n",
      "Precision: 0.79\n",
      "Recall: 0.87\n",
      "F1 Score: 0.83\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\DELL\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:183: FutureWarning: max_iter and tol parameters have been added in SGDClassifier in 0.19. If max_iter is set but tol is left unset, the default value for tol in 0.19 and 0.20 will be None (which is equivalent to -infinity, so it has no effect) but will change in 0.21 to 1e-3. Specify tol to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "dt_model = DecisionTreeClassifier()\n",
    "dt_avgwv_predictions = train_predict_evaluate_model(classifier=svm,\n",
    "                                           train_features=avg_wv_train_features,\n",
    "                                           train_labels=train_labels,\n",
    "                                           test_features=avg_wv_test_features,\n",
    "                                           test_labels=test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>278</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>34</td>\n",
       "      <td>256</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     0    1\n",
       "0  278    4\n",
       "1   34  256"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cm_dt = metrics.confusion_matrix(test_labels, dt_avgwv_predictions)\n",
    "pd.DataFrame(cm, index=range(0,2), columns=range(0,2))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
